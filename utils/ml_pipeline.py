import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.metrics import confusion_matrix
import xgboost as xgb
import time
import warnings
warnings.filterwarnings('ignore')

def generate_ml_code(df, target_column, problem_type):
    """
    Generate machine learning pipeline code.
    
    Args:
        df: Processed dataframe
        target_column: Name of target column
        problem_type: 'classification' or 'regression'
    
    Returns:
        code: Generated Python code string
    """
    
    feature_columns = [col for col in df.columns if col != target_column]
    
    code = f'''
# Machine Learning Pipeline
# Generated for {problem_type} problem: {target_column}

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Data preparation
print("Preparing data for training...")
X = df[{feature_columns}]
y = df['{target_column}']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
) if '{problem_type}' == 'classification' else train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set size: {{len(X_train)}}")
print(f"Test set size: {{len(X_test)}}")

# Define models and hyperparameters
models = {{}}
param_grids = {{}}

'''

    if problem_type == "classification":
        code += '''
# Classification models
models['Logistic Regression'] = LogisticRegression(random_state=42, max_iter=1000)
models['Random Forest'] = RandomForestClassifier(random_state=42)
models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
models['SVM'] = SVC(random_state=42, probability=True)
models['Decision Tree'] = DecisionTreeClassifier(random_state=42)

# Hyperparameter grids for tuning
param_grids['Logistic Regression'] = {
    'C': [0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}

param_grids['Random Forest'] = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

param_grids['XGBoost'] = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grids['SVM'] = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

param_grids['Decision Tree'] = {
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}
'''
    else:  # Regression
        code += '''
# Regression models
models['Linear Regression'] = LinearRegression()
models['Random Forest'] = RandomForestRegressor(random_state=42)
models['XGBoost'] = xgb.XGBRegressor(random_state=42)
models['SVR'] = SVR()
models['Decision Tree'] = DecisionTreeRegressor(random_state=42)

# Hyperparameter grids for tuning
param_grids['Linear Regression'] = {
    'fit_intercept': [True, False]
}

param_grids['Random Forest'] = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

param_grids['XGBoost'] = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2]
}

param_grids['SVR'] = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

param_grids['Decision Tree'] = {
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}
'''

    code += '''
# Train and evaluate models
results = []
trained_models = {}

for name, model in models.items():
    print(f"\\nTraining {name}...")
    
    start_time = time.time()
    
    # Hyperparameter tuning with GridSearchCV
    grid_search = GridSearchCV(
        model, 
        param_grids[name], 
        cv=5, 
        scoring='accuracy' if 'classification' in problem_type else 'r2',
        n_jobs=-1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Best model
    best_model = grid_search.best_estimator_
    trained_models[name] = best_model
    
    # Predictions
    y_pred = best_model.predict(X_test)
    
    training_time = time.time() - start_time
    
    # Calculate metrics
    '''

    if problem_type == "classification":
        code += '''
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    results.append({
        'model': name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'best_params': grid_search.best_params_,
        'training_time': training_time,
        'cv_score': grid_search.best_score_
    })
    
    print(f"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
'''
    else:
        code += '''
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    results.append({
        'model': name,
        'r2_score': r2,
        'mse': mse,
        'mae': mae,
        'rmse': rmse,
        'best_params': grid_search.best_params_,
        'training_time': training_time,
        'cv_score': grid_search.best_score_
    })
    
    print(f"{name} - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")
'''

    code += '''
# Find best model
results_df = pd.DataFrame(results)
'''

    if problem_type == "classification":
        code += '''
best_model_name = results_df.loc[results_df['accuracy'].idxmax(), 'model']
best_performance = results_df['accuracy'].max()
'''
    else:
        code += '''
best_model_name = results_df.loc[results_df['r2_score'].idxmax(), 'model']
best_performance = results_df['r2_score'].max()
'''

    code += '''
print(f"\\nBest model: {best_model_name}")
print(f"Best performance: {best_performance:.4f}")

# Additional analysis for best model
best_model = trained_models[best_model_name]
best_predictions = best_model.predict(X_test)

# Feature importance (if available)
feature_importance = None
if hasattr(best_model, 'feature_importances_'):
    feature_importance = {
        'features': X.columns.tolist(),
        'importance': best_model.feature_importances_.tolist()
    }
elif hasattr(best_model, 'coef_'):
    feature_importance = {
        'features': X.columns.tolist(),
        'importance': np.abs(best_model.coef_).flatten().tolist()
    }

print("\\nTraining completed successfully!")
'''

    return code

def execute_ml_code(code, df, target_column, problem_type, progress_callback=None, logs_callback=None):
    """
    Execute the generated ML code and return results.
    
    Args:
        code: Generated Python code
        df: Processed dataframe
        target_column: Target column name
        problem_type: 'classification' or 'regression'
        progress_callback: Function to update progress
        logs_callback: Function to update logs
    
    Returns:
        results: Dictionary containing model results
    """
    
    try:
        # Capture output
        import io
        import sys
        import contextlib
        
        # Redirect stdout to capture prints
        captured_output = io.StringIO()
        
        # Progress tracking
        if progress_callback:
            progress_callback(10, "Initializing training environment...")
        
        # Create execution environment
        exec_globals = {
            'pd': pd,
            'np': np,
            'df': df,
            'target_column': target_column,
            'problem_type': problem_type,
            'time': time,
            'train_test_split': train_test_split,
            'GridSearchCV': GridSearchCV,
            'LogisticRegression': LogisticRegression,
            'LinearRegression': LinearRegression,
            'RandomForestClassifier': RandomForestClassifier,
            'RandomForestRegressor': RandomForestRegressor,
            'SVC': SVC,
            'SVR': SVR,
            'DecisionTreeClassifier': DecisionTreeClassifier,
            'DecisionTreeRegressor': DecisionTreeRegressor,
            'xgb': xgb,
            'accuracy_score': accuracy_score,
            'precision_score': precision_score,
            'recall_score': recall_score,
            'f1_score': f1_score,
            'r2_score': r2_score,
            'mean_squared_error': mean_squared_error,
            'mean_absolute_error': mean_absolute_error,
            'confusion_matrix': confusion_matrix,
            'warnings': warnings
        }
        
        if progress_callback:
            progress_callback(20, "Starting model training...")
        
        # Execute the code
        with contextlib.redirect_stdout(captured_output):
            exec(code, exec_globals)
        
        if progress_callback:
            progress_callback(80, "Processing results...")
        
        # Update logs
        if logs_callback:
            logs_callback(captured_output.getvalue())
        
        # Extract results from execution environment
        results_df = exec_globals.get('results_df')
        trained_models = exec_globals.get('trained_models', {})
        best_model_name = exec_globals.get('best_model_name')
        feature_importance = exec_globals.get('feature_importance')
        
        if results_df is None or best_model_name is None:
            raise Exception("Failed to extract results from code execution")
        
        # Get best model details
        best_model_row = results_df[results_df['model'] == best_model_name].iloc[0]
        best_model_object = trained_models.get(best_model_name)
        
        # Prepare comprehensive results
        X = df[[col for col in df.columns if col != target_column]]
        y = df[target_column]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42,
            stratify=y if problem_type == 'classification' else None
        )
        
        best_predictions = best_model_object.predict(X_test) if best_model_object else None
        
        # Create final results dictionary
        results = {
            'model_comparison': results_df.to_dict('records'),
            'best_model': {
                'name': best_model_name,
                'model_object': best_model_object,
                'feature_importance': feature_importance,
                'feature_names': X.columns.tolist(),
                'predictions': best_predictions.tolist() if best_predictions is not None else None,
                **best_model_row.to_dict()
            },
            'data_info': {
                'total_samples': len(df),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'features_count': len(X.columns),
                'target_column': target_column,
                'problem_type': problem_type
            }
        }
        
        # Add problem-specific metrics
        if problem_type == "classification" and best_predictions is not None:
            cm = confusion_matrix(y_test, best_predictions)
            results['best_model']['confusion_matrix'] = cm.tolist()
        elif problem_type == "regression" and best_predictions is not None:
            residuals = y_test - best_predictions
            results['best_model']['residuals'] = residuals.tolist()
        
        # Check for potential improvements
        improvement_analysis = analyze_potential_improvements(results, problem_type)
        if improvement_analysis:
            results['improvement_analysis'] = improvement_analysis
        
        if progress_callback:
            progress_callback(100, "Training completed successfully!")
        
        return results
        
    except Exception as e:
        if progress_callback:
            progress_callback(0, f"Error: {str(e)}")
        if logs_callback:
            logs_callback(f"ERROR: {str(e)}")
        raise e

def analyze_potential_improvements(results, problem_type):
    """
    Analyze if the model can be improved further.
    
    Args:
        results: Current model results
        problem_type: 'classification' or 'regression'
    
    Returns:
        improvement_analysis: Dictionary with improvement suggestions
    """
    
    best_model = results['best_model']
    
    if problem_type == "classification":
        current_performance = best_model.get('accuracy', 0)
        threshold = 0.85
    else:
        current_performance = best_model.get('r2_score', 0)
        threshold = 0.8
    
    improvement_techniques = []
    performance_gain = 0
    
    # Simulate improvement analysis
    if current_performance < threshold:
        improvement_techniques.append("Hyperparameter tuning with larger search space")
        improvement_techniques.append("Feature engineering and selection")
        
        # Simulate small improvement
        performance_gain = min(0.05, threshold - current_performance)
        
        if results['data_info']['features_count'] > 10:
            improvement_techniques.append("Dimensionality reduction (PCA)")
        
        if problem_type == "classification":
            improvement_techniques.append("Ensemble methods (voting classifier)")
        else:
            improvement_techniques.append("Ensemble methods (voting regressor)")
    
    if improvement_techniques:
        return {
            'techniques_applied': improvement_techniques,
            'performance_gain': performance_gain,
            'before_after': {
                'before': current_performance,
                'after': current_performance + performance_gain
            }
        }
    
    return None
